{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy library\nimport numpy as np\n\n# numpy array\narray = [[1,2,3],[4,5,6]]\nfirst_array = np.array(array) # 2x3 array\nprint(\"Array Type: {}\".format(type(first_array))) # type\nprint(\"Array Shape: {}\".format(np.shape(first_array))) # shape\nprint(first_array)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pytorch library\nimport torch\n\n# pytorch array\ntensor = torch.Tensor(array)\nprint(\"Array Type: {}\".format(tensor.type)) # type\nprint(\"Array Shape: {}\".format(tensor.shape)) # shape\nprint(tensor)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# numpy ones\nprint(\"Numpy {}\\n\".format(np.ones((2,3))))\n\n# pytorch ones\nprint(torch.ones((2,3)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# numpy random\nprint(\"Numpy {}\\n\".format(np.random.rand(2,3)))\n\n# pytorch random\nprint(torch.rand(2,3))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# random numpy array\narray = np.random.rand(2,2)\nprint(\"{} {}\\n\".format(type(array),array))\n\n# from numpy to tensor\nfrom_numpy_to_tensor = torch.from_numpy(array)\nprint(\"{}\\n\".format(from_numpy_to_tensor))\n\n# from tensor to numpy\ntensor = from_numpy_to_tensor\nfrom_tensor_to_numpy = tensor.numpy()\nprint(\"{} {}\\n\".format(type(from_tensor_to_numpy),from_tensor_to_numpy))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create tensor \ntensor = torch.ones(3,3)\nprint(\"\\n\",tensor)\n\n# Resize\nprint(\"{}{}\\n\".format(tensor.view(9).shape,tensor.view(9)))\n\n# Addition\nprint(\"Addition: {}\\n\".format(torch.add(tensor,tensor)))\n\n# Subtraction\nprint(\"Subtraction: {}\\n\".format(tensor.sub(tensor)))\n\n# Element wise multiplication\nprint(\"Element wise multiplication: {}\\n\".format(torch.mul(tensor,tensor)))\n\n# Element wise division\nprint(\"Element wise division: {}\\n\".format(torch.div(tensor,tensor)))\n\n# Mean\ntensor = torch.Tensor([1,2,3,4,5])\nprint(\"Mean: {}\".format(tensor.mean()))\n\n# Standart deviation (std)\nprint(\"std: {}\".format(tensor.std()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import variable from pytorch library\nfrom torch.autograd import Variable\n\n# define variable\nvar = Variable(torch.ones(3), requires_grad = True)\nvar","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets make basic backward propagation\n# we have an equation that is y = x^2\narray = [2,4]\ntensor = torch.Tensor(array)\nx = Variable(tensor, requires_grad = True)\ny = x**2\nprint(\" y =  \",y)\n\n# recap o equation o = 1/2*sum(y)\no = (1/2)*sum(y)\nprint(\" o =  \",o)\n\n# backward\no.backward() # calculates gradients\n\n# As I defined, variables accumulates gradients. In this part there is only one variable x.\n# Therefore variable x should be have gradients\n# Lets look at gradients with x.grad\nprint(\"gradients: \",x.grad)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As a car company we collect this data from previous selling\n# lets define car prices\ncar_prices_array = [3,4,5,6,7,8,9]\ncar_price_np = np.array(car_prices_array,dtype=np.float32)\ncar_price_np = car_price_np.reshape(-1,1)\ncar_price_tensor = Variable(torch.from_numpy(car_price_np))\n\n# lets define number of car sell\nnumber_of_car_sell_array = [ 7.5, 7, 6.5, 6.0, 5.5, 5.0, 4.5]\nnumber_of_car_sell_np = np.array(number_of_car_sell_array,dtype=np.float32)\nnumber_of_car_sell_np = number_of_car_sell_np.reshape(-1,1)\nnumber_of_car_sell_tensor = Variable(torch.from_numpy(number_of_car_sell_np))\n\n# lets visualize our data\nimport matplotlib.pyplot as plt\nplt.scatter(car_prices_array,number_of_car_sell_array)\nplt.xlabel(\"Car Price $\")\nplt.ylabel(\"Number of Car Sell\")\nplt.title(\"Car Price$ VS Number of Car Sell\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Linear Regression with Pytorch\n\n# libraries\nimport torch      \nfrom torch.autograd import Variable     \nimport torch.nn as nn \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# create class\nclass LinearRegression(nn.Module):\n    def __init__(self,input_size,output_size):\n        # super function. It inherits from nn.Module and we can access everythink in nn.Module\n        super(LinearRegression,self).__init__()\n        # Linear function.\n        self.linear = nn.Linear(input_dim,output_dim)\n\n    def forward(self,x):\n        return self.linear(x)\n    \n# define model\ninput_dim = 1\noutput_dim = 1\nmodel = LinearRegression(input_dim,output_dim) # input and output size are 1\n\n# MSE\nmse = nn.MSELoss()\n\n# Optimization (find parameters that minimize error)\nlearning_rate = 0.02   # how fast we reach best parameters\noptimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n\n# train model\nloss_list = []\niteration_number = 1001\nfor iteration in range(iteration_number):\n        \n    # optimization\n    optimizer.zero_grad() \n    \n    # Forward to get output\n    results = model(car_price_tensor)\n    \n    # Calculate Loss\n    loss = mse(results, number_of_car_sell_tensor)\n    \n    # backward propagation\n    loss.backward()\n    \n    # Updating parameters\n    optimizer.step()\n    \n    # store loss\n    loss_list.append(loss.data)\n    \n    # print loss\n    if(iteration % 50 == 0):\n        print('epoch {}, loss {}'.format(iteration, loss.data))\n\nplt.plot(range(iteration_number),loss_list)\nplt.xlabel(\"Number of Iterations\")\nplt.ylabel(\"Loss\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict our car price \npredicted = model(car_price_tensor).data.numpy()\nplt.scatter(car_prices_array,number_of_car_sell_array,label = \"original data\",color =\"red\")\nplt.scatter(car_prices_array,predicted,label = \"predicted data\",color =\"blue\")\n\n# predict if car price is 10$, what will be the number of car sell\n#predicted_10 = model(torch.from_numpy(np.array([10]))).data.numpy()\n#plt.scatter(10,predicted_10.data,label = \"car price 10$\",color =\"green\")\nplt.legend()\nplt.xlabel(\"Car Price $\")\nplt.ylabel(\"Number of Car Sell\")\nplt.title(\"Original vs Predicted values\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Libraries\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare Dataset\n# load data\ntrain = pd.read_csv(r\"../input/train.csv\",dtype = np.float32)\n\n# split data into features(pixels) and labels(numbers from 0 to 9)\ntargets_numpy = train.label.values\nfeatures_numpy = train.loc[:,train.columns != \"label\"].values/255 # normalization\n\n# train test split. Size of train data is 80% and size of test data is 20%. \nfeatures_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n                                                                             targets_numpy,\n                                                                             test_size = 0.2,\n                                                                             random_state = 42) \n\n# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\nfeaturesTrain = torch.from_numpy(features_train)\ntargetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n\n# create feature and targets tensor for test set.\nfeaturesTest = torch.from_numpy(features_test)\ntargetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n\n# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 10000\nnum_epochs = n_iters / (len(features_train) / batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n\n# visualize one of the images in data set\nplt.imshow(features_numpy[10].reshape(28,28))\nplt.axis(\"off\")\nplt.title(str(targets_numpy[10]))\nplt.savefig('graph.png')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Logistic Regression Model\nclass LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegressionModel, self).__init__()\n        # Linear part\n        self.linear = nn.Linear(input_dim, output_dim)\n        # There should be logistic function right?\n        # However logistic function in pytorch is in loss function\n        # So actually we do not forget to put it, it is only at next parts\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n# Instantiate Model Class\ninput_dim = 28*28 # size of image px*px\noutput_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n\n# create logistic regression model\nmodel = LogisticRegressionModel(input_dim, output_dim)\n\n# Cross Entropy Loss  \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer \nlearning_rate = 0.001\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Traning the Model\ncount = 0\nloss_list = []\niteration_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        # Define variables\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and cross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculate gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        # Prediction\n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader: \n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct / float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {}%'.format(count, loss.data, accuracy))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualization\nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Logistic Regression: Loss vs Number of iteration\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Libraries\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create ANN Model\nclass ANNModel(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ANNModel, self).__init__()\n        \n        # Linear function 1: 784 --> 150\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity 1\n        self.relu1 = nn.ReLU()\n        \n        # Linear function 2: 150 --> 150\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 2\n        self.tanh2 = nn.Tanh()\n        \n        # Linear function 3: 150 --> 150\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 3\n        self.elu3 = nn.ELU()\n        \n        # Linear function 4 (readout): 150 --> 10\n        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n    \n    def forward(self, x):\n        # Linear function 1\n        out = self.fc1(x)\n        # Non-linearity 1\n        out = self.relu1(out)\n        \n        # Linear function 2\n        out = self.fc2(out)\n        # Non-linearity 2\n        out = self.tanh2(out)\n        \n        # Linear function 2\n        out = self.fc3(out)\n        # Non-linearity 2\n        out = self.elu3(out)\n        \n        # Linear function 4 (readout)\n        out = self.fc4(out)\n        return out\n\n# instantiate ANN\ninput_dim = 28*28\nhidden_dim = 150 #hidden layer dim is one of the hyper parameter and it should be chosen and tuned. For now I only say 150 there is no reason.\noutput_dim = 10\n\n# Create ANN\nmodel = ANNModel(input_dim, hidden_dim, output_dim)\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.02\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ANN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader:\n\n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct / float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"ANN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ANN: Accuracy vs Number of iteration\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Libraries\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CNN Model\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        \n        # Convolution 1\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n        self.relu1 = nn.ReLU()\n        \n        # Max pool 1\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n     \n        # Convolution 2\n        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n        self.relu2 = nn.ReLU()\n        \n        # Max pool 2\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n        \n        # Fully connected 1\n        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n    \n    def forward(self, x):\n        # Convolution 1\n        out = self.cnn1(x)\n        out = self.relu1(out)\n        \n        # Max pool 1\n        out = self.maxpool1(out)\n        \n        # Convolution 2 \n        out = self.cnn2(out)\n        out = self.relu2(out)\n        \n        # Max pool 2 \n        out = self.maxpool2(out)\n        \n        # flatten\n        out = out.view(out.size(0), -1)\n\n        # Linear function (readout)\n        out = self.fc1(out)\n        \n        return out\n\n# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 2500\nnum_epochs = n_iters / (len(features_train) / batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)\n    \n# Create CNN\nmodel = CNNModel()\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CNN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        train = Variable(images.view(100,1,28,28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                \n                test = Variable(images.view(100,1,28,28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct / float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"CNN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"CNN: Accuracy vs Number of iteration\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]}]}