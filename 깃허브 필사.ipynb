{"metadata":{"kernelspec":{"display_name":"Python [default]","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Batch Normalization in PyTorch","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import datasets\nimport torchvision.transforms as transforms\n\n# number of subprocesses to use for data loading\nnum_workers = 0\n# how many samples per batch to load\nbatch_size = 64\n\n# convert data to torch.FloatTensor\ntransform = transforms.ToTensor()\n\n# get the training and test datasets\ntrain_data = datasets.MNIST(root='data', train=True,\n                            download=True, transform=transform)\n\ntest_data = datasets.MNIST(root='data', train=False,\n                           download=True, transform=transform)\n\n# prepare data loaders\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n                                           num_workers=num_workers)\n\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n                                          num_workers=num_workers)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obtain one batch of training images\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy()\n\n# get one image from the batch\nimg = np.squeeze(images[0])\n\nfig = plt.figure(figsize = (3,3)) \nax = fig.add_subplot(111)\nax.imshow(img, cmap='gray')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass NeuralNet(nn.Module):\n    def __init__(self, use_batch_norm, input_size=784, hidden_dim=256, output_size=10):\n        \"\"\"\n        Creates a PyTorch net using the given parameters.\n        \n        :param use_batch_norm: bool\n            Pass True to create a network that uses batch normalization; False otherwise\n            Note: this network will not use batch normalization on layers that do not have an\n            activation function.\n        \"\"\"\n        super(NeuralNet, self).__init__() # init super\n        \n        # Default layer sizes\n        self.input_size = input_size # (28*28 images)\n        self.hidden_dim = hidden_dim\n        self.output_size = output_size # (number of classes)\n        # Keep track of whether or not this network uses batch normalization.\n        self.use_batch_norm = use_batch_norm\n        \n        # define hidden linear layers, with optional batch norm on their outputs\n        # layers with batch_norm applied have no bias term\n        if use_batch_norm:\n            self.fc1 = nn.Linear(input_size, hidden_dim*2, bias=False)\n            self.batch_norm1 = nn.BatchNorm1d(hidden_dim*2)\n        else:\n            self.fc1 = nn.Linear(input_size, hidden_dim*2)\n            \n        # define *second* hidden linear layers, with optional batch norm on their outputs\n        if use_batch_norm:\n            self.fc2 = nn.Linear(hidden_dim*2, hidden_dim, bias=False)\n            self.batch_norm2 = nn.BatchNorm1d(hidden_dim)\n        else:\n            self.fc2 = nn.Linear(hidden_dim*2, hidden_dim)\n        \n        # third and final, fully-connected layer\n        self.fc3 = nn.Linear(hidden_dim, output_size)\n        \n        \n    def forward(self, x):\n        # flatten image\n        x = x.view(-1, 28*28)\n        # all hidden layers + optional batch norm + relu activation\n        x = self.fc1(x)\n        if self.use_batch_norm:\n            x = self.batch_norm1(x)\n        x = F.relu(x)\n        # second layer\n        x = self.fc2(x)\n        if self.use_batch_norm:\n            x = self.batch_norm2(x)\n        x = F.relu(x)\n        # third layer, no batch norm or activation\n        x = self.fc3(x)\n        return x\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net_batchnorm = NeuralNet(use_batch_norm=True)\nnet_no_norm = NeuralNet(use_batch_norm=False)\n\nprint(net_batchnorm)\nprint()\nprint(net_no_norm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, n_epochs=10):\n    # number of epochs to train the model\n    n_epochs = n_epochs\n    # track losses\n    losses = []\n        \n    # optimization strategy \n    # specify loss function (categorical cross-entropy)\n    criterion = nn.CrossEntropyLoss()\n\n    # specify optimizer (stochastic gradient descent) and learning rate = 0.01\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n    # set the model to training mode\n    model.train()\n    \n    for epoch in range(1, n_epochs+1):\n        # monitor training loss\n        train_loss = 0.0\n\n        ###################\n        # train the model #\n        ###################\n        batch_count = 0\n        for batch_idx, (data, target) in enumerate(train_loader):\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the loss\n            loss = criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # update average training loss\n            train_loss += loss.item() # add up avg batch loss\n            batch_count +=1                \n\n        # print training statistics \n        losses.append(train_loss/batch_count)\n        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n            epoch, \n            train_loss/batch_count))\n    \n    # return all recorded batch losses\n    return losses\n        \n      ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batchnorm model losses\n# this may take some time to train\nlosses_batchnorm = train(net_batchnorm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# *no* norm model losses\n# you should already start to see a difference in training losses\nlosses_no_norm = train(net_no_norm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare\nfig, ax = plt.subplots(figsize=(12,8))\n#losses_batchnorm = np.array(losses_batchnorm)\n#losses_no_norm = np.array(losses_no_norm)\nplt.plot(losses_batchnorm, label='Using batchnorm', alpha=0.5)\nplt.plot(losses_no_norm, label='No norm', alpha=0.5)\nplt.title(\"Training Losses\")\nplt.legend()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model, train):\n    # initialize vars to monitor test loss and accuracy\n    class_correct = list(0. for i in range(10))\n    class_total = list(0. for i in range(10))\n    test_loss = 0.0\n\n    # set model to train or evaluation mode\n    # just to see the difference in behavior\n    if(train==True):\n        model.train()\n    if(train==False):\n        model.eval()\n    \n    # loss criterion\n    criterion = nn.CrossEntropyLoss()\n    \n    for batch_idx, (data, target) in enumerate(test_loader):\n        batch_size = data.size(0)\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the loss\n        loss = criterion(output, target)\n        # update average test loss \n        test_loss += loss.item()*batch_size\n        # convert output probabilities to predicted class\n        _, pred = torch.max(output, 1)\n        # compare predictions to true label\n        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n        # calculate test accuracy for each object class\n        for i in range(batch_size):\n            label = target.data[i]\n            class_correct[label] += correct[i].item()\n            class_total[label] += 1\n\n    print('Test Loss: {:.6f}\\n'.format(test_loss/len(test_loader.dataset)))\n\n    for i in range(10):\n        if class_total[i] > 0:\n            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n                str(i), 100 * class_correct[i] / class_total[i],\n                np.sum(class_correct[i]), np.sum(class_total[i])))\n        else:\n            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n\n    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n        100. * np.sum(class_correct) / np.sum(class_total),\n        np.sum(class_correct), np.sum(class_total)))","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test batchnorm case, in *train* mode\ntest(net_batchnorm, train=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test batchnorm case, in *evaluation* mode\ntest(net_batchnorm, train=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for posterity, test no norm case in eval mode\ntest(net_no_norm, train=False)","metadata":{},"execution_count":null,"outputs":[]}]}